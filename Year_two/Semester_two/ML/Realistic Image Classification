# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input/competitie'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
In [2]:
# Pas 1: citirea datelor din fisier:

import os
import numpy as np
from PIL import Image
from sklearn.preprocessing import LabelEncoder

# Definim calea către directoarele cu imagini
train_dir = '/kaggle/input/dataset/train'
test_dir = '/kaggle/input/dataset/test'
validation_dir = '/kaggle/input/dataset/validation'

# Funcție pentru a încărca imaginile și a le transforma în array-uri
def load_images_from_folder(folder, image_size=(128, 128)):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = Image.open(img_path).resize(image_size)
        img_array = np.array(img)
        # Verifică dacă imaginea are dimensiunea corectă
        if img_array.shape == (image_size[0], image_size[1], 3):
            images.append(img_array)
    return np.array(images)

# Încărcăm imaginile de antrenament, test și validare
X_train = load_images_from_folder(train_dir)
X_test = load_images_from_folder(test_dir)
X_validation = load_images_from_folder(validation_dir)

# Verificăm forma array-urilor de imagini
print(f"Train images shape: {X_train.shape}")
print(f"Test images shape: {X_test.shape}")
print(f"Validation images shape: {X_validation.shape}")
Train images shape: (10495, 128, 128, 3)
Test images shape: (4499, 128, 128, 3)
Validation images shape: (2999, 128, 128, 3)
In [3]:
# Pas 2: prelucrarea datelor din fisier

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import load_img, img_to_array
from sklearn.preprocessing import LabelEncoder

# Functie pentru citirea și redimensionarea imaginilor
def load_and_preprocess_images(folder, img_height, img_width):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        if img_path.endswith('.png'):
            label = filename.split('_')[0]
            img = load_img(img_path, target_size=(img_height, img_width))
            img_array = img_to_array(img)
            images.append(img_array)
            labels.append(label)
    return np.array(images), np.array(labels)

# Căile către directoarele de date
train_dir = '/kaggle/input/dataset/train'  # Înlocuiește cu calea ta reală
validation_dir = '/kaggle/input/dataset/validation'  # Înlocuiește cu calea ta reală

# Dimensiunile imaginilor
img_height = 128
img_width = 128

# Citim și preprocesăm imaginile
X_train, train_labels = load_and_preprocess_images(train_dir, img_height, img_width)
X_validation, validation_labels = load_and_preprocess_images(validation_dir, img_height, img_width)

# Codificăm etichetele
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(train_labels)

# Filtrăm etichetele de validare pentru a se asigura că toate etichetele sunt prezente în setul de antrenament
filtered_validation_labels = [label for label in validation_labels if label in train_labels]
filtered_validation_images = [img for img, label in zip(X_validation, validation_labels) if label in train_labels]

# Transformăm doar etichetele de validare care sunt prezente în setul de antrenament
y_validation = label_encoder.transform(filtered_validation_labels)

# Convertim în array pentru TensorFlow
X_validation = np.array(filtered_validation_images)

# Verificăm dimensiunile array-urilor
print(f"Dimensiune X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Dimensiune X_validation: {X_validation.shape}, y_validation: {y_validation.shape}")
2024-06-07 16:25:49.165727: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-06-07 16:25:49.165841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-06-07 16:25:49.323506: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
Dimensiune X_train: (10500, 128, 128, 3), y_train: (10500,)
Dimensiune X_validation: (0,), y_validation: (0,)
In [4]:
# Pas 3: Construirea pipeline-ului de inceput

# Creăm seturile de date TensorFlow
batch_size = 32

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = tf.data.Dataset.from_tensor_slices((X_validation, y_validation)).batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)
In [5]:
# Pas 4: Construirea si antrenarea modeluli

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

# Definim modelul
num_classes = len(label_encoder.classes_)

model = Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])

# Compilăm modelul
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Antrenăm modelul
epochs = 10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
Epoch 1/10
/opt/conda/lib/python3.10/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:18: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
 18/329 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.0000e+00 - loss: 9.2631 
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1717777591.739123      73 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
W0000 00:00:1717777591.757586      73 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
 99/329 ━━━━━━━━━━━━━━━━━━━━ 7s 31ms/step - accuracy: 0.0000e+00 - loss: 9.2635
W0000 00:00:1717777594.650747      72 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update
327/329 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - accuracy: 0.0000e+00 - loss: 9.2799
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[5], line 29
     27 # Antrenăm modelul
     28 epochs = 10
---> 29 history = model.fit(
     30   train_ds,
     31   validation_data=val_ds,
     32   epochs=epochs
     33 )

File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File /opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py:288, in Functional._adjust_input_rank(self, flat_inputs)
    286             adjusted.append(ops.expand_dims(x, axis=-1))
    287             continue
--> 288     raise ValueError(
    289         f"Invalid input shape for input {x}. Expected shape "
    290         f"{ref_shape}, but input has incompatible shape {x.shape}"
    291     )
    292 # Add back metadata.
    293 for i in range(len(flat_inputs)):

ValueError: Exception encountered when calling Sequential.call().

Invalid input shape for input Tensor("Cast:0", shape=(None,), dtype=float32). Expected shape (None, 128, 128, 3), but input has incompatible shape (None,)

Arguments received by Sequential.call():
  • inputs=tf.Tensor(shape=(None,), dtype=float32)
  • training=False
  • mask=None
In [ ]:
# Pas 5: Evaluarea si validarea modelului:

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

# Definim modelul
num_classes = len(label_encoder.classes_)

model = Sequential([
    layers.Input(shape=(img_height, img_width, 3)),  # Folosim un strat Input separat
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])

# Compilăm modelul
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Antrenăm modelul
epochs = 10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)
In [ ]:
# Imbunatatirea modelului:

# Augmentarea datelor
data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal", input_shape=(img_height, img_width, 3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)

# Redefinim modelul cu augmentarea datelor și dropout
model = Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
])

# Compilăm și antrenăm modelul din nou
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

# Evaluăm modelul pe setul de test
test_loss, test_acc = model.evaluate(test_ds, verbose=2)
print(f"\nTest accuracy: {test_acc}")

# Vizualizăm din nou rezultatele antrenării
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()
